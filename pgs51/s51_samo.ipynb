{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    MinMaxScaler,\n",
    "    OneHotEncoder,\n",
    "    LabelEncoder,\n",
    "    label_binarize,\n",
    "    OrdinalEncoder,\n",
    ")\n",
    "from category_encoders import CatBoostEncoder, MEstimateEncoder\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    VotingClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    HistGradientBoostingRegressor,\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    RidgeClassifier,\n",
    "    LogisticRegression,\n",
    "    LinearRegression,\n",
    "    Ridge,\n",
    "    BayesianRidge,\n",
    ")\n",
    "\n",
    "from sklearn import set_config\n",
    "import os\n",
    "\n",
    "import optuna\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    mean_squared_error,\n",
    "    precision_recall_curve,\n",
    "    make_scorer,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    RocCurveDisplay,\n",
    "    matthews_corrcoef,\n",
    "    mean_absolute_percentage_error,\n",
    ")\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "from colorama import Fore, Style, init\n",
    "from copy import deepcopy\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    RepeatedStratifiedKFold,\n",
    "    StratifiedKFold,\n",
    "    KFold,\n",
    "    RepeatedKFold,\n",
    "    cross_val_score,\n",
    "    StratifiedGroupKFold,\n",
    "    GroupKFold,\n",
    ")\n",
    "from xgboost import DMatrix, XGBClassifier, XGBRegressor\n",
    "from lightgbm import (\n",
    "    log_evaluation,\n",
    "    early_stopping,\n",
    "    LGBMClassifier,\n",
    "    LGBMRegressor,\n",
    "    Dataset,\n",
    ")\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
    "from tqdm.notebook import tqdm\n",
    "from optuna.samplers import TPESampler, CmaEsSampler\n",
    "from optuna.pruners import HyperbandPruner\n",
    "from functools import partial\n",
    "from IPython.display import display_html, clear_output\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import gc\n",
    "import re\n",
    "import holidays\n",
    "import requests\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "sns.set_style(\"whitegrid\")\n",
    "import warnings\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn._oldcore\")\n",
    "\n",
    "model_round_ = 4\n",
    "# os.listdir()\n",
    "class Config:\n",
    "    state = 52943\n",
    "    n_splits = 5\n",
    "    early_stop = 200\n",
    "    target = \"num_sold\"\n",
    "    train = pd.concat(\n",
    "        [\n",
    "            pd.read_csv(\"./datadown/train.csv\", index_col=\"id\"),\n",
    "            pd.read_csv(\"./dataup/f_X_extra_features_nomodel.csv\", index_col=0),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    tfs =  pd.read_csv(\"./dataup/f_test_extra_features_nomodel.csv\", index_col=0)\n",
    "    tfs.index = range(230130,230130+98550)\n",
    "    test = pd.concat(\n",
    "        [\n",
    "            pd.read_csv(\"./datadown/test.csv\", index_col=\"id\"),\n",
    "            tfs\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "    submission = pd.read_csv(\"./datadown/sample_submission.csv\")\n",
    "    original_data = \"N\"\n",
    "    outliers = \"N\"\n",
    "    log_trf = \"Y\"\n",
    "    scaler_trf = \"N\"\n",
    "    feature_eng = \"Y\"\n",
    "    missing = \"Y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_df = pd.read_csv(\"/kaggle/input/playground-series-s5e1/train.csv\", parse_dates=[\"date\"])\n",
    "    test_df = pd.read_csv(\"/kaggle/input/playground-series-s5e1/test.csv\", parse_dates=[\"date\"])\n",
    "except:\n",
    "    train_df = pd.read_csv(\"./datadown/train.csv\", parse_dates=[\"date\"])\n",
    "    test_df = pd.read_csv(\"./datadown/test.csv\", parse_dates=[\"date\"])\n",
    "gdp_per_capita_df = pd.read_csv(\"./datadown/gdp_per_capita.csv\")\n",
    "years =  [\"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"]\n",
    "gdp_per_capita_filtered_df = gdp_per_capita_df.loc[gdp_per_capita_df[\"Country Name\"].isin(train_df[\"country\"].unique()), [\"Country Name\"] + years].set_index(\"Country Name\")\n",
    "gdp_per_capita_filtered_df[\"2010_ratio\"] = gdp_per_capita_filtered_df[\"2010\"] / gdp_per_capita_filtered_df.sum()[\"2010\"]\n",
    "for year in years:\n",
    "    gdp_per_capita_filtered_df[f\"{year}_ratio\"] = gdp_per_capita_filtered_df[year] / gdp_per_capita_filtered_df.sum()[year]\n",
    "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_df[[i+\"_ratio\" for i in years]]\n",
    "gdp_per_capita_filtered_ratios_df.columns = [int(i) for i in years]\n",
    "gdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_ratios_df.unstack().reset_index().rename(columns = {\"level_0\": \"year\", 0: \"ratio\", \"Country Name\": \"country\"})\n",
    "gdp_per_capita_filtered_ratios_df['year'] = pd.to_datetime(gdp_per_capita_filtered_ratios_df['year'], format='%Y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X[\n",
    "#     [\n",
    "#         \"day_of_week_ratios\",\n",
    "#         \"country_ratio\",\n",
    "#         \"product_ratio\",\n",
    "#         \"store_ratio\",\n",
    "#         \"num_sold_pseudo_label\",\n",
    "#     ]\n",
    "# ].to_csv('./dataup/f_X_extra_features_nomodel.csv')\n",
    "# nomodel_sub[\n",
    "#     [\n",
    "#         \"day_of_week_ratios\",\n",
    "#         # \"adjusted_num_sold\",\n",
    "#         \"country_ratio\",\n",
    "#         \"product_ratio\",\n",
    "#         \"store_ratio\",\n",
    "#         \"num_sold\",\n",
    "#     ]\n",
    "# ].to_csv('./dataup/f_test_extra_features_nomodel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transform(Config):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        if Config.original_data == \"Y\":\n",
    "            self.train = pd.concat(\n",
    "                [self.train, self.train_org], ignore_index=True\n",
    "            ).drop_duplicates()\n",
    "            self.train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        self.num_features = (\n",
    "            self.train.drop(self.target, axis=1)\n",
    "            .select_dtypes(exclude=[\"object\", \"bool\"])\n",
    "            .columns.tolist()\n",
    "        )\n",
    "        self.cat_features = (\n",
    "            self.train.drop(self.target, axis=1)\n",
    "            .select_dtypes(include=[\"object\", \"bool\"])\n",
    "            .columns.tolist()\n",
    "        )\n",
    "        self.train_raw = self.train.copy()\n",
    "        # self.num_features = list(set(self.num_features + cbcs_))\n",
    "\n",
    "        if self.feature_eng == \"Y\":\n",
    "            self.train = self.new_features(self.train)\n",
    "            self.test = self.new_features(self.test)\n",
    "            self.train_raw = self.new_features(self.train_raw)\n",
    "\n",
    "        if self.missing == \"Y\":\n",
    "            self.missing_values()\n",
    "\n",
    "        self.num_features = (\n",
    "            self.train.drop(self.target, axis=1)\n",
    "            .select_dtypes(exclude=[\"object\", \"bool\"])\n",
    "            .columns.tolist()\n",
    "        )\n",
    "        self.cat_features = (\n",
    "            self.train.drop(self.target, axis=1)\n",
    "            .select_dtypes(include=[\"object\", \"bool\"])\n",
    "            .columns.tolist()\n",
    "        )\n",
    "        if self.outliers == \"Y\":\n",
    "            self.remove_outliers()\n",
    "\n",
    "        if self.log_trf == \"Y\":\n",
    "            self.log_transformation()\n",
    "\n",
    "        if self.scaler_trf == \"Y\":\n",
    "            self.scaler()\n",
    "\n",
    "        self.train_enc = self.train.copy()\n",
    "        self.test_enc = self.test.copy()\n",
    "        self.encode()\n",
    "\n",
    "        # if self.outliers == \"Y\" or self.log_trf == \"Y\" or self.scaler_trf == \"Y\":\n",
    "        #     self.distribution()\n",
    "\n",
    "    def __call__(self):\n",
    "\n",
    "        self.train[self.cat_features] = self.train[self.cat_features].astype(\"category\")\n",
    "        self.test[self.cat_features] = self.test[self.cat_features].astype(\"category\")\n",
    "\n",
    "        self.cat_features_card = []\n",
    "        for f in self.cat_features:\n",
    "            self.cat_features_card.append(self.train[f].nunique())\n",
    "\n",
    "        self.train = self.reduce_mem(self.train)\n",
    "        self.test = self.reduce_mem(self.test)\n",
    "\n",
    "        self.y = self.train[self.target]\n",
    "        self.train = self.train.drop(self.target, axis=1)\n",
    "        self.train_enc = self.train_enc.drop(self.target, axis=1)\n",
    "\n",
    "        return (\n",
    "            self.train,\n",
    "            self.train_enc,\n",
    "            self.y,\n",
    "            self.test,\n",
    "            self.test_enc,\n",
    "            self.cat_features,\n",
    "        )\n",
    "\n",
    "    def encode(self):\n",
    "        data = pd.concat([self.test, self.train], axis = 0)\n",
    "        oe = OrdinalEncoder()\n",
    "        data[self.cat_features] = oe.fit_transform(data[self.cat_features]).astype(\n",
    "            \"int\"\n",
    "        )\n",
    "        # print(self.num_features)\n",
    "        scaler = StandardScaler()\n",
    "        data[self.num_features + [self.target]] = scaler.fit_transform(\n",
    "            data[self.num_features + [self.target]]\n",
    "        )\n",
    "\n",
    "        self.train_enc = data[~data[self.target].isna()]\n",
    "        self.test_enc = data[data[self.target].isna()].drop(self.target, axis=1)\n",
    "\n",
    "\n",
    "    def new_features(self, data):\n",
    "        data[\"date\"] = pd.to_datetime(data[\"date\"])\n",
    "        data[\"quarter\"] = data[\"date\"].dt.quarter.astype(\"object\")\n",
    "        data[\"month\"] = data[\"date\"].dt.month\n",
    "        data[\"month_sin\"] = np.sin(data[\"month\"] * (2 * np.pi / 12))\n",
    "        data[\"month_cos\"] = np.cos(data[\"month\"] * (2 * np.pi / 12))\n",
    "        data[\"day\"] = data[\"date\"].dt.day\n",
    "        data[\"day_sin\"] = np.sin(data[\"day\"] * (2 * np.pi / 31))\n",
    "        data[\"day_cos\"] = np.cos(data[\"day\"] * (2 * np.pi / 31))\n",
    "        data[\"day_of_week\"] = data[\"date\"].dt.dayofweek.astype(\"object\")\n",
    "        data[\"day_of_year\"] = data[\"date\"].dt.dayofyear.astype(\"object\")\n",
    "        # data[\"day_of_week\"] = data[\"day_of_week\"].apply(\n",
    "        #     lambda x: 0 if x <= 3 else (1 if x == 4 else (2 if x == 5 else (3)))\n",
    "        # )\n",
    "        data[\"week\"] = data[\"date\"].dt.isocalendar().week\n",
    "        data[\"week_sin\"] = np.sin(data[\"week\"] * (2 * np.pi / 53))\n",
    "        data[\"week_cos\"] = np.cos(data[\"week\"] * (2 * np.pi / 53))\n",
    "        data[\"year\"] = data[\"date\"].dt.year\n",
    "        data[\"cos_year\"] = np.cos(data[\"year\"] * (2 * np.pi) / 100)\n",
    "        data[\"sin_year\"] = np.sin(data[\"year\"] * (2 * np.pi) / 100)\n",
    "        data[[\"month\", \"day\", \"week\", \"year\"]] = data[\n",
    "            [\"month\", \"day\", \"week\", \"year\"]\n",
    "        ].astype(\"object\")\n",
    "        data[\"group\"] = (\n",
    "            (data[\"year\"] - 2020) * 48 + data[\"month\"] * 4 + data[\"day\"] // 7\n",
    "        )\n",
    "        data = pd.get_dummies(data, columns=[\"day_of_week\"], drop_first=True)\n",
    "        data[\"important_dates\"] = data[\"day_of_year\"].apply(\n",
    "            lambda x: (\n",
    "                x\n",
    "                if x\n",
    "                in [\n",
    "                    1,\n",
    "                    2,\n",
    "                    3,\n",
    "                    4,\n",
    "                    5,\n",
    "                    6,\n",
    "                    7,\n",
    "                    8,\n",
    "                    9,\n",
    "                    10,\n",
    "                    99,\n",
    "                    100,\n",
    "                    101,\n",
    "                    125,\n",
    "                    126,\n",
    "                    355,\n",
    "                    256,\n",
    "                    357,\n",
    "                    358,\n",
    "                    359,\n",
    "                    360,\n",
    "                    361,\n",
    "                    362,\n",
    "                    363,\n",
    "                    364,\n",
    "                    365,\n",
    "                ]\n",
    "                else 0\n",
    "            )\n",
    "        )\n",
    "        data.drop(columns=[\"day_of_year\"], inplace=True)\n",
    "        for day in range(24, 32):\n",
    "            data[f\"dec{day}\"] = (\n",
    "                (data.date.dt.day.eq(day) & data.date.dt.month.eq(12))\n",
    "                .astype(np.uint8)\n",
    "                .astype(\"object\")\n",
    "            )\n",
    "        alpha2 = dict(\n",
    "            zip(np.sort(data.country.unique()), [\"CA\", \"FI\", \"IT\", \"KE\", \"NO\", \"SG\"])\n",
    "        )\n",
    "        h = {\n",
    "            c: holidays.country_holidays(a, years=range(2010, 2020))\n",
    "            for c, a in alpha2.items()\n",
    "        }\n",
    "        data[\"is_holiday\"] = 0\n",
    "        for c in alpha2:\n",
    "            data.loc[data.country == c, \"is_holiday\"] = (\n",
    "                data.date.isin(h[c]).astype(int).astype(\"object\")\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            gdp_df = pd.read_csv(\n",
    "                \"./datadown/gdp_df.csv\"\n",
    "            )  # .drop('Unnamed: 0', axis = 1)\n",
    "        except:\n",
    "            print(\"no gdp data\")\n",
    "            gdp = []\n",
    "            for country in data.country.unique():\n",
    "                row = []\n",
    "                for year in range(2010, 2020):\n",
    "                    row.append(self.get_gdp(country, year))\n",
    "                gdp.append(row)\n",
    "\n",
    "            gdp = np.array(gdp)\n",
    "            gdp /= np.sum(gdp, axis=0)\n",
    "\n",
    "            gdp_df = pd.DataFrame(\n",
    "                gdp, index=data.country.unique(), columns=range(2010, 2020)\n",
    "            )\n",
    "            gdp_df = (\n",
    "                gdp_df.reset_index()\n",
    "                .melt(id_vars=[\"index\"])\n",
    "                .rename(\n",
    "                    {\"index\": \"country\", \"variable\": \"year\", \"value\": \"GDP\"}, axis=1\n",
    "                )\n",
    "            )\n",
    "        data = data.merge(gdp_df, how=\"left\", on=[\"year\", \"country\"])\n",
    "        return data\n",
    "\n",
    "    def log_transformation(self):\n",
    "        self.train[self.target] = np.log1p(self.train[self.target])\n",
    "        return self\n",
    "\n",
    "    def distribution(self):\n",
    "        print(Style.BRIGHT + Fore.GREEN + f\"\\nHistograms of distribution\\n\")\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "        ax_r, ax_n = axes\n",
    "\n",
    "        ax_r.set_title(\n",
    "            f\"{self.target} ($\\mu=$ {self.train_raw[self.target].mean():.2f} and $\\sigma=$ {self.train_raw[self.target].std():.2f} )\"\n",
    "        )\n",
    "        ax_r.hist(self.train_raw[self.target], bins=30, color=\"#3cb371\")\n",
    "        ax_r.axvline(self.train_raw[self.target].mean(), color=\"r\", label=\"Mean\")\n",
    "        ax_r.axvline(\n",
    "            self.train_raw[self.target].median(),\n",
    "            color=\"y\",\n",
    "            linestyle=\"--\",\n",
    "            label=\"Median\",\n",
    "        )\n",
    "        ax_r.legend()\n",
    "\n",
    "        ax_n.set_title(\n",
    "            f\"{self.target} Normalized ($\\mu=$ {self.train_enc[self.target].mean():.2f} and $\\sigma=$ {self.train_enc[self.target].std():.2f} )\"\n",
    "        )\n",
    "        ax_n.hist(self.train_enc[self.target], bins=30, color=\"#3cb371\")\n",
    "        ax_n.axvline(self.train_enc[self.target].mean(), color=\"r\", label=\"Mean\")\n",
    "        ax_n.axvline(\n",
    "            self.train_enc[self.target].median(),\n",
    "            color=\"y\",\n",
    "            linestyle=\"--\",\n",
    "            label=\"Median\",\n",
    "        )\n",
    "        ax_n.legend()\n",
    "\n",
    "    def remove_outliers(self):\n",
    "        Q1 = self.train[self.targets].quantile(0.25)\n",
    "        Q3 = self.train[self.targets].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_limit = Q1 - 1.5 * IQR\n",
    "        upper_limit = Q3 + 1.5 * IQR\n",
    "        self.train = self.train[\n",
    "            (self.train[self.targets] >= lower_limit)\n",
    "            & (self.train[self.targets] <= upper_limit)\n",
    "        ]\n",
    "        self.train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    def scaler(self, cols: [] = None):\n",
    "        scaler = StandardScaler()\n",
    "        self.train[cols] = scaler.fit_transform(self.train[cols])\n",
    "        self.test[cols] = scaler.transform(self.test[cols])\n",
    "        return self\n",
    "\n",
    "    def missing_values(self):\n",
    "        train_df_imputed = self.train.copy()\n",
    "        for year in train_df_imputed[\"year\"].unique():\n",
    "            target_ratio = train_df_imputed.loc[\n",
    "                (train_df_imputed[\"year\"] == year)\n",
    "                & (train_df_imputed[\"country\"] == \"Norway\"),\n",
    "                \"GDP\",\n",
    "            ].values[0]\n",
    "            current_raito = train_df_imputed.loc[\n",
    "                (train_df_imputed[\"year\"] == year)\n",
    "                & (train_df_imputed[\"country\"] == \"Canada\"),\n",
    "                \"GDP\",\n",
    "            ].values[0]\n",
    "            ratio_can = current_raito / target_ratio\n",
    "            train_df_imputed.loc[\n",
    "                (train_df_imputed[\"country\"] == \"Canada\")\n",
    "                & (train_df_imputed[\"store\"] == \"Discount Stickers\")\n",
    "                & (train_df_imputed[\"product\"] == \"Holographic Goose\")\n",
    "                & (train_df_imputed[\"year\"] == year),\n",
    "                \"num_sold\",\n",
    "            ] = (\n",
    "                train_df_imputed.loc[\n",
    "                    (train_df_imputed[\"country\"] == \"Norway\")\n",
    "                    & (train_df_imputed[\"store\"] == \"Discount Stickers\")\n",
    "                    & (train_df_imputed[\"product\"] == \"Holographic Goose\")\n",
    "                    & (train_df_imputed[\"year\"] == year),\n",
    "                    \"num_sold\",\n",
    "                ]\n",
    "                * ratio_can\n",
    "            ).values\n",
    "\n",
    "            current_ts = train_df_imputed.loc[\n",
    "                (train_df_imputed[\"country\"] == \"Canada\")\n",
    "                & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\")\n",
    "                & (train_df_imputed[\"product\"] == \"Holographic Goose\")\n",
    "                & (train_df_imputed[\"year\"] == year)\n",
    "            ]\n",
    "            missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "            train_df_imputed.loc[\n",
    "                (train_df_imputed[\"country\"] == \"Canada\")\n",
    "                & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\")\n",
    "                & (train_df_imputed[\"product\"] == \"Holographic Goose\")\n",
    "                & (train_df_imputed[\"year\"] == year)\n",
    "                & (train_df_imputed[\"date\"].isin(missing_ts_dates)),\n",
    "                \"num_sold\",\n",
    "            ] = (\n",
    "                train_df_imputed.loc[\n",
    "                    (train_df_imputed[\"country\"] == \"Norway\")\n",
    "                    & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\")\n",
    "                    & (train_df_imputed[\"product\"] == \"Holographic Goose\")\n",
    "                    & (train_df_imputed[\"year\"] == year)\n",
    "                    & (train_df_imputed[\"date\"].isin(missing_ts_dates)),\n",
    "                    \"num_sold\",\n",
    "                ]\n",
    "                * ratio_can\n",
    "            ).values\n",
    "\n",
    "            current_ts = train_df_imputed.loc[\n",
    "                (train_df_imputed[\"country\"] == \"Canada\")\n",
    "                & (train_df_imputed[\"store\"] == \"Stickers for Less\")\n",
    "                & (train_df_imputed[\"product\"] == \"Holographic Goose\")\n",
    "                & (train_df_imputed[\"year\"] == year)\n",
    "            ]\n",
    "            missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "            train_df_imputed.loc[\n",
    "                (train_df_imputed[\"country\"] == \"Canada\")\n",
    "                & (train_df_imputed[\"store\"] == \"Stickers for Less\")\n",
    "                & (train_df_imputed[\"product\"] == \"Holographic Goose\")\n",
    "                & (train_df_imputed[\"year\"] == year)\n",
    "                & (train_df_imputed[\"date\"].isin(missing_ts_dates)),\n",
    "                \"num_sold\",\n",
    "            ] = (\n",
    "                train_df_imputed.loc[\n",
    "                    (train_df_imputed[\"country\"] == \"Norway\")\n",
    "                    & (train_df_imputed[\"store\"] == \"Stickers for Less\")\n",
    "                    & (train_df_imputed[\"product\"] == \"Holographic Goose\")\n",
    "                    & (train_df_imputed[\"year\"] == year)\n",
    "                    & (train_df_imputed[\"date\"].isin(missing_ts_dates)),\n",
    "                    \"num_sold\",\n",
    "                ]\n",
    "                * ratio_can\n",
    "            ).values\n",
    "\n",
    "            current_raito = train_df_imputed.loc[\n",
    "                (train_df_imputed[\"year\"] == year)\n",
    "                & (train_df_imputed[\"country\"] == \"Kenya\"),\n",
    "                \"GDP\",\n",
    "            ].values[0]\n",
    "            ratio_ken = current_raito / target_ratio\n",
    "            train_df_imputed.loc[\n",
    "                (train_df_imputed[\"country\"] == \"Kenya\")\n",
    "                & (train_df_imputed[\"store\"] == \"Discount Stickers\")\n",
    "                & (train_df_imputed[\"product\"] == \"Holographic Goose\")\n",
    "                & (train_df_imputed[\"year\"] == year),\n",
    "                \"num_sold\",\n",
    "            ] = (\n",
    "                train_df_imputed.loc[\n",
    "                    (train_df_imputed[\"country\"] == \"Norway\")\n",
    "                    & (train_df_imputed[\"store\"] == \"Discount Stickers\")\n",
    "                    & (train_df_imputed[\"product\"] == \"Holographic Goose\")\n",
    "                    & (train_df_imputed[\"year\"] == year),\n",
    "                    \"num_sold\",\n",
    "                ]\n",
    "                * ratio_ken\n",
    "            ).values\n",
    "\n",
    "            current_ts = train_df_imputed.loc[\n",
    "                (train_df_imputed[\"country\"] == \"Kenya\")\n",
    "                & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\")\n",
    "                & (train_df_imputed[\"product\"] == \"Holographic Goose\")\n",
    "                & (train_df_imputed[\"year\"] == year)\n",
    "            ]\n",
    "            missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "            train_df_imputed.loc[\n",
    "                (train_df_imputed[\"country\"] == \"Kenya\")\n",
    "                & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\")\n",
    "                & (train_df_imputed[\"product\"] == \"Holographic Goose\")\n",
    "                & (train_df_imputed[\"year\"] == year)\n",
    "                & (train_df_imputed[\"date\"].isin(missing_ts_dates)),\n",
    "                \"num_sold\",\n",
    "            ] = (\n",
    "                train_df_imputed.loc[\n",
    "                    (train_df_imputed[\"country\"] == \"Norway\")\n",
    "                    & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\")\n",
    "                    & (train_df_imputed[\"product\"] == \"Holographic Goose\")\n",
    "                    & (train_df_imputed[\"year\"] == year)\n",
    "                    & (train_df_imputed[\"date\"].isin(missing_ts_dates)),\n",
    "                    \"num_sold\",\n",
    "                ]\n",
    "                * ratio_ken\n",
    "            ).values\n",
    "\n",
    "            current_ts = train_df_imputed.loc[\n",
    "                (train_df_imputed[\"country\"] == \"Kenya\")\n",
    "                & (train_df_imputed[\"store\"] == \"Stickers for Less\")\n",
    "                & (train_df_imputed[\"product\"] == \"Holographic Goose\")\n",
    "                & (train_df_imputed[\"year\"] == year)\n",
    "            ]\n",
    "            missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "            train_df_imputed.loc[\n",
    "                (train_df_imputed[\"country\"] == \"Kenya\")\n",
    "                & (train_df_imputed[\"store\"] == \"Stickers for Less\")\n",
    "                & (train_df_imputed[\"product\"] == \"Holographic Goose\")\n",
    "                & (train_df_imputed[\"year\"] == year)\n",
    "                & (train_df_imputed[\"date\"].isin(missing_ts_dates)),\n",
    "                \"num_sold\",\n",
    "            ] = (\n",
    "                train_df_imputed.loc[\n",
    "                    (train_df_imputed[\"country\"] == \"Norway\")\n",
    "                    & (train_df_imputed[\"store\"] == \"Stickers for Less\")\n",
    "                    & (train_df_imputed[\"product\"] == \"Holographic Goose\")\n",
    "                    & (train_df_imputed[\"year\"] == year)\n",
    "                    & (train_df_imputed[\"date\"].isin(missing_ts_dates)),\n",
    "                    \"num_sold\",\n",
    "                ]\n",
    "                * ratio_ken\n",
    "            ).values\n",
    "\n",
    "            current_ts = train_df_imputed.loc[\n",
    "                (train_df_imputed[\"country\"] == \"Kenya\")\n",
    "                & (train_df_imputed[\"store\"] == \"Discount Stickers\")\n",
    "                & (train_df_imputed[\"product\"] == \"Kerneler\")\n",
    "                & (train_df_imputed[\"year\"] == year)\n",
    "            ]\n",
    "            missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n",
    "            train_df_imputed.loc[\n",
    "                (train_df_imputed[\"country\"] == \"Kenya\")\n",
    "                & (train_df_imputed[\"store\"] == \"Discount Stickers\")\n",
    "                & (train_df_imputed[\"product\"] == \"Kerneler\")\n",
    "                & (train_df_imputed[\"year\"] == year)\n",
    "                & (train_df_imputed[\"date\"].isin(missing_ts_dates)),\n",
    "                \"num_sold\",\n",
    "            ] = (\n",
    "                train_df_imputed.loc[\n",
    "                    (train_df_imputed[\"country\"] == \"Norway\")\n",
    "                    & (train_df_imputed[\"store\"] == \"Discount Stickers\")\n",
    "                    & (train_df_imputed[\"product\"] == \"Kerneler\")\n",
    "                    & (train_df_imputed[\"year\"] == year)\n",
    "                    & (train_df_imputed[\"date\"].isin(missing_ts_dates)),\n",
    "                    \"num_sold\",\n",
    "                ]\n",
    "                * ratio_ken\n",
    "            ).values\n",
    "\n",
    "        train_df_imputed.loc[train_df_imputed.index == 23719, \"num_sold\"] = 4\n",
    "        train_df_imputed.loc[train_df_imputed.index == 207003, \"num_sold\"] = 195\n",
    "        self.train = train_df_imputed.drop(\"date\", axis=1)\n",
    "        self.test.drop(\"date\", axis=1, inplace=True)\n",
    "        # * samo mutate year to int.,\n",
    "        self.train[\"year\"] = self.train[\"year\"].astype(int) - 2010\n",
    "        self.test[\"year\"] = self.test[\"year\"].astype(int) - 2010\n",
    "        self.train[\"group\"] = self.train[\"group\"].astype(int)\n",
    "        self.test[\"group\"] = self.test[\"group\"].astype(int)\n",
    "        return self\n",
    "\n",
    "    def reduce_mem(self, df):\n",
    "\n",
    "        numerics = [\n",
    "            \"int16\",\n",
    "            \"int32\",\n",
    "            \"int64\",\n",
    "            \"float16\",\n",
    "            \"float32\",\n",
    "            \"float64\",\n",
    "            \"uint16\",\n",
    "            \"uint32\",\n",
    "            \"uint64\",\n",
    "        ]\n",
    "\n",
    "        for col in df.columns:\n",
    "            col_type = df[col].dtypes\n",
    "\n",
    "            if col_type in numerics:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "\n",
    "                if \"int\" in str(col_type):\n",
    "                    if c_min >= np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif (\n",
    "                        c_min >= np.iinfo(np.int16).min\n",
    "                        and c_max < np.iinfo(np.int16).max\n",
    "                    ):\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif (\n",
    "                        c_min >= np.iinfo(np.int32).min\n",
    "                        and c_max < np.iinfo(np.int32).max\n",
    "                    ):\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif (\n",
    "                        c_min >= np.iinfo(np.int64).min\n",
    "                        and c_max < np.iinfo(np.int64).max\n",
    "                    ):\n",
    "                        df[col] = df[col].astype(np.int64)\n",
    "                else:\n",
    "                    if (\n",
    "                        c_min >= np.finfo(np.float16).min\n",
    "                        and c_max < np.finfo(np.float16).max\n",
    "                    ):\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    if (\n",
    "                        c_min >= np.finfo(np.float32).min\n",
    "                        and c_max < np.finfo(np.float32).max\n",
    "                    ):\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float64)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_gdp(self, country, year):\n",
    "        alpha3 = {\n",
    "            \"Canada\": \"CAN\",\n",
    "            \"Finland\": \"FIN\",\n",
    "            \"Italy\": \"ITA\",\n",
    "            \"Kenya\": \"KEN\",\n",
    "            \"Norway\": \"NOR\",\n",
    "            \"Singapore\": \"SGP\",\n",
    "        }\n",
    "\n",
    "        url = \"https://api.worldbank.org/v2/country/{0}/indicator/NY.GDP.PCAP.CD?date={1}&format=json\".format(\n",
    "            alpha3[country], year\n",
    "        )\n",
    "        response = requests.get(url).json()\n",
    "        return response[1][0][\"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 sec  \n",
    "t = Transform()\n",
    "X, X_enc, y, test, test_enc, cat_features = t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for j in range(len(cat_features)):\n",
    "    # print(j)\n",
    "    unique_values = test_enc[cat_features[j]].unique()\n",
    "    # print(f\"Feature {cat_features[j]}: Unique values = {unique_values}\")\n",
    "    assert min(unique_values) >= 0, ''\n",
    "    assert max(unique_values) < t.cat_features_card[j], ''\n",
    "t.cat_features == cat_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import contextlib, io\n",
    "import ydf\n",
    "\n",
    "ydf.verbose(2)\n",
    "from ydf import GradientBoostedTreesLearner\n",
    "\n",
    "\n",
    "def YDFRegressor(learner_class):\n",
    "\n",
    "    class YDFXRegressor(BaseEstimator, RegressorMixin):\n",
    "\n",
    "        def __init__(self, params={}):\n",
    "            self.params = params\n",
    "\n",
    "        def fit(self, X, y):\n",
    "            assert isinstance(X, pd.DataFrame)\n",
    "            assert isinstance(y, pd.Series)\n",
    "            target = y.name\n",
    "            params = self.params.copy()\n",
    "            params[\"label\"] = target\n",
    "            params[\"task\"] = ydf.Task.REGRESSION\n",
    "            X = pd.concat([X, y], axis=1)\n",
    "            with contextlib.redirect_stderr(io.StringIO()), contextlib.redirect_stdout(\n",
    "                io.StringIO()\n",
    "            ):\n",
    "                self.model = learner_class(**params).train(X)\n",
    "            return self\n",
    "\n",
    "        def predict(self, X):\n",
    "            assert isinstance(X, pd.DataFrame)\n",
    "            with contextlib.redirect_stderr(io.StringIO()), contextlib.redirect_stdout(\n",
    "                io.StringIO()\n",
    "            ):\n",
    "                return self.model.predict(X)\n",
    "\n",
    "    return YDFXRegressor\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    \n",
    "    x_input_cats = layers.Input(shape=(len(cat_features),))\n",
    "    embs = []\n",
    "    for j in range(len(cat_features)):\n",
    "        e = layers.Embedding(\n",
    "            t.cat_features_card[j], int(np.ceil(np.sqrt(t.cat_features_card[j])))\n",
    "        )\n",
    "        x = e(x_input_cats[:, j])\n",
    "        x = layers.Flatten()(x)\n",
    "        embs.append(x)\n",
    "    [print(i) for i in embs]\n",
    "    x_input_nums = layers.Input(shape=(len(t.num_features),))\n",
    "    x = layers.Concatenate(axis=-1)(embs + [x_input_nums])\n",
    "    x = layers.Dense(512, activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(512, activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(512, activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "\n",
    "    model = keras.Model(inputs=[x_input_cats, x_input_nums], outputs=x)\n",
    "    return model\n",
    "# LSTM-based model\n",
    "def build_model_lstm():\n",
    "    x_input_cats = layers.Input(shape=(len(cat_features),))\n",
    "    embs = []\n",
    "    for j in range(len(cat_features)):\n",
    "        e = layers.Embedding(t.cat_features_card[j], int(np.ceil(np.sqrt(t.cat_features_card[j]))))\n",
    "        x = e(x_input_cats[:,j])\n",
    "        x = layers.Flatten()(x)\n",
    "        embs.append(x)\n",
    "        \n",
    "    x_input_nums = layers.Input(shape=(len(t.num_features),))\n",
    "    \n",
    "    # Combine embeddings and numerical features\n",
    "    x = layers.Concatenate(axis=-1)(embs+[x_input_nums])\n",
    "    # Reshape for LSTM (adding time dimension)\n",
    "    x = layers.Reshape((1, -1))(x)\n",
    "    # LSTM layers\n",
    "    x = layers.LSTM(256, return_sequences=True)(x)\n",
    "    x = layers.LSTM(128)(x)\n",
    "    # Dense layers\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "    \n",
    "    model = keras.Model(inputs=[x_input_cats, x_input_nums], outputs=x)\n",
    "    return model\n",
    "\n",
    "# Self-attention based model\n",
    "def build_model_attention():\n",
    "    x_input_cats = layers.Input(shape=(len(cat_features),))\n",
    "    embs = []\n",
    "    for j in range(len(cat_features)):\n",
    "        e = layers.Embedding(t.cat_features_card[j], int(np.ceil(np.sqrt(t.cat_features_card[j]))))\n",
    "        x = e(x_input_cats[:,j])\n",
    "        x = layers.Flatten()(x)\n",
    "        embs.append(x)\n",
    "        \n",
    "    x_input_nums = layers.Input(shape=(len(t.num_features),))\n",
    "    # Combine embeddings and numerical features\n",
    "    x = layers.Concatenate(axis=-1)(embs+[x_input_nums])\n",
    "    # Multi-head self attention\n",
    "    def attention_block(x, num_heads=4):\n",
    "        dim = x.shape[-1]\n",
    "        hidden_dim = dim // num_heads\n",
    "        # Reshape for multi-head attention\n",
    "        x = layers.Reshape((1, -1))(x)\n",
    "        # Self attention layer\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, \n",
    "            key_dim=hidden_dim\n",
    "        )(x, x)\n",
    "        \n",
    "        # Add & Norm\n",
    "        x = layers.Add()([x, attention_output])\n",
    "        x = layers.LayerNormalization()(x)\n",
    "        \n",
    "        # Flatten back\n",
    "        x = layers.Flatten()(x)\n",
    "        return x\n",
    "    # Apply attention\n",
    "    x = attention_block(x)\n",
    "    # Dense layers\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "    \n",
    "    model = keras.Model(inputs=[x_input_cats, x_input_nums], outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# models = {\n",
    "#     \"NN1\": [build_model, True],\n",
    "#     \"NN2\": [build_model_lstm, True],\n",
    "#     \"NN3\": [build_model_attention, True],\n",
    "# }\n",
    "models = {\n",
    "    'NN': [_,\n",
    "           False],\n",
    "    # 'CAT': [CatBoostRegressor(**{'verbose': 0,\n",
    "    #                              'random_state': Config.state,\n",
    "    #                              'cat_features': cat_features,\n",
    "    #                              'early_stopping_rounds': Config.early_stop,\n",
    "    #                              'eval_metric': \"RMSE\",\n",
    "    #                              'n_estimators' : 2000,\n",
    "    #                              'depth': 3,\n",
    "    #                              'min_data_in_leaf': 96,\n",
    "    #                              'l2_leaf_reg': 8.972890275248485,\n",
    "    #                              'bagging_temperature': 0.18658249870341914, \n",
    "    #                              'random_strength': 0.14106593468982453,\n",
    "    #                              'learning_rate': 0.01,\n",
    "    #                             }),\n",
    "    #         True],\n",
    "    # 'CAT3': [CatBoostRegressor(**{'verbose': 0,\n",
    "    #                               'random_state': Config.state,\n",
    "    #                               'cat_features': cat_features,\n",
    "    #                               'early_stopping_rounds': Config.early_stop,\n",
    "    #                               'eval_metric': \"MAPE\",\n",
    "    #                               'n_estimators' : 2000,\n",
    "    #                               'learning_rate': 0.01,\n",
    "    #                               'depth': 8,\n",
    "    #                               'min_data_in_leaf': 99,\n",
    "    #                               'l2_leaf_reg': 7.7324870113971125, \n",
    "    #                               'bagging_temperature': 0.003232535109945575, \n",
    "    #                               'random_strength': 0.12145610701952099,\n",
    "    #                              }),\n",
    "    #          True],\n",
    "    # 'CAT5': [CatBoostRegressor(**{'depth': 7,\n",
    "    #                               'min_data_in_leaf': 59,\n",
    "    #                               'l2_leaf_reg': 6.485681470975604, \n",
    "    #                               'bagging_temperature': 0.728613892125684,\n",
    "    #                               'random_strength': 0.3565990691132947,\n",
    "    #                               'verbose': 0,\n",
    "    #                               'random_state': Config.state,\n",
    "    #                               'cat_features': cat_features,\n",
    "    #                               'early_stopping_rounds': Config.early_stop,\n",
    "    #                               'eval_metric': \"MAPE\",\n",
    "    #                               'n_estimators' : 2000,\n",
    "    #                               'learning_rate': 0.01,\n",
    "    #                               \"task_type\": \"cpu\",\n",
    "    #                              }),\n",
    "    #          True],\n",
    "    # 'CAT6': [CatBoostRegressor(**{'depth': 10,\n",
    "    #                               'min_data_in_leaf': 67,\n",
    "    #                               'l2_leaf_reg': 0.010658988402410939,\n",
    "    #                               'bagging_temperature': 0.7381549501573549,\n",
    "    #                               'random_strength': 0.10057316762567874,\n",
    "    #                               'verbose': 0,\n",
    "    #                               'random_state': Config.state,\n",
    "    #                               'cat_features': cat_features,\n",
    "    #                               'early_stopping_rounds': Config.early_stop,\n",
    "    #                               'eval_metric': \"MAPE\",\n",
    "    #                               'n_estimators' : 2000,\n",
    "    #                               'learning_rate': 0.01,\n",
    "    #                               'bootstrap_type': 'Poisson',\n",
    "    #                               \"task_type\": \"cpu\",\n",
    "    #                              }),\n",
    "    #          True],\n",
    "    # 'XGB2': [XGBRegressor(**{'tree_method': 'hist',\n",
    "    #                          'n_estimators': 2000,\n",
    "    #                          'objective': 'reg:squarederror',\n",
    "    #                          'random_state': Config.state,\n",
    "    #                          'enable_categorical': True,\n",
    "    #                          'verbosity': 0,\n",
    "    #                          'early_stopping_rounds': Config.early_stop,\n",
    "    #                          'eval_metric': 'rmse',\n",
    "    #                          'booster': 'gbtree', \n",
    "    #                          'max_depth': 3,\n",
    "    #                          'min_child_weight': 16,\n",
    "    #                          'subsample': 0.8172380854733758, \n",
    "    #                          'reg_alpha': 0.2734696712123178, \n",
    "    #                          'reg_lambda': 0.5865768393479154,\n",
    "    #                          'colsample_bytree': 0.9766164536195251,\n",
    "    #                          'n_jobs': -1,\n",
    "    #                          'learning_rate': 0.01,\n",
    "    #                          'n_jobs': -1\n",
    "    #                         }),\n",
    "    #         True],\n",
    "    # 'XGB3': [XGBRegressor(**{'tree_method': 'hist',\n",
    "    #                          'n_estimators': 2000,\n",
    "    #                          'learning_rate': 0.01,\n",
    "    #                          'objective': 'reg:squarederror',\n",
    "    #                          'random_state': Config.state,\n",
    "    #                          'enable_categorical': True,\n",
    "    #                          'verbosity': 0,\n",
    "    #                          'early_stopping_rounds': Config.early_stop,\n",
    "    #                          'eval_metric': 'mape',\n",
    "    #                          'booster': 'gbtree',\n",
    "    #                          'max_depth': 3,\n",
    "    #                          'min_child_weight': 12,\n",
    "    #                          'subsample': 0.7720667996291699, \n",
    "    #                          'reg_alpha': 0.07869714859026081, \n",
    "    #                          'reg_lambda': 0.9577219578640989, \n",
    "    #                          'colsample_bytree': 0.9728085969282255, \n",
    "    #                          'n_jobs': -1\n",
    "    #                        }),\n",
    "    #     True],\n",
    "    # 'XGB4': [XGBRegressor(**{'booster': 'gbtree',\n",
    "    #                          'max_depth': 3,\n",
    "    #                          'min_child_weight': 12,\n",
    "    #                          'subsample': 0.800221370346261,\n",
    "    #                          'reg_alpha': 0.4571249607822852,\n",
    "    #                          'reg_lambda': 0.6572354640280187,\n",
    "    #                          'colsample_bytree': 0.9982441671154363,\n",
    "    #                          'n_jobs': -1,\n",
    "    #                          'tree_method': 'hist',\n",
    "    #                          'n_estimators': 3000,\n",
    "    #                          'learning_rate': 0.01,\n",
    "    #                          'objective': 'reg:squarederror',\n",
    "    #                          'random_state': Config.state,\n",
    "    #                          'enable_categorical': True,\n",
    "    #                          'verbosity': 0,\n",
    "    #                          'early_stopping_rounds': Config.early_stop,\n",
    "    #                          'eval_metric': 'mape',\n",
    "    #                          'booster': 'gbtree',\n",
    "    #                          \"device\": \"cuda\",\n",
    "    #                         }),\n",
    "    #          True],\n",
    "    # 'XGB5': [XGBRegressor(**{'booster': 'gbtree',\n",
    "    #                          'max_depth': 3,\n",
    "    #                          'min_child_weight': 19,\n",
    "    #                          'subsample': 0.8065343833518619,\n",
    "    #                          'reg_alpha': 0.3577049940509907,\n",
    "    #                          'reg_lambda': 0.8560297700871249,\n",
    "    #                          'colsample_bytree': 0.9866141987520272,\n",
    "    #                          'objective': 'reg:squarederror',\n",
    "    #                          'n_jobs': -1,\n",
    "    #                          'tree_method': 'hist',\n",
    "    #                          'n_estimators': 3000,\n",
    "    #                          'learning_rate': 0.01,\n",
    "    #                          'random_state': Config.state,\n",
    "    #                          'enable_categorical': True,\n",
    "    #                          'verbosity': 0,\n",
    "    #                          'early_stopping_rounds': Config.early_stop,\n",
    "    #                          'eval_metric': 'mape', \n",
    "    #                          \"device\": \"cuda\",\n",
    "    #                          }),\n",
    "    #          True],\n",
    "    'LGBM2': [LGBMRegressor(**{'random_state': Config.state,\n",
    "                               'early_stopping_round': Config.early_stop,\n",
    "                               'categorical_feature': cat_features,\n",
    "                               'verbose': -1,\n",
    "                               'boosting_type': 'gbdt',\n",
    "                               'eval_metric': 'rmse',\n",
    "                               'objective': 'regression_l2',\n",
    "                               'n_estimators': 5000,\n",
    "                               'max_depth': 13, \n",
    "                               'num_leaves': 891, \n",
    "                               'min_child_samples': 16,\n",
    "                               'min_child_weight': 11,\n",
    "                               'colsample_bytree': 0.48639630433139497,\n",
    "                               'reg_alpha': 0.45496760242817474,\n",
    "                               'reg_lambda': 0.9669296995303693,\n",
    "                               'learning_rate': 0.01\n",
    "                              }),\n",
    "             False],\n",
    "    'LGBM3': [LGBMRegressor(**{'random_state': Config.state,\n",
    "                               'early_stopping_round': Config.early_stop,\n",
    "                               'categorical_feature': cat_features,\n",
    "                               'verbose': -1,\n",
    "                               'boosting_type': 'gbdt',\n",
    "                               'eval_metric': 'rmse',\n",
    "                               'objective': 'regression_l2',\n",
    "                               'n_estimators': 2000,\n",
    "                               'max_depth': 6, \n",
    "                               'num_leaves': 328,\n",
    "                               'min_child_samples': 10,\n",
    "                               'min_child_weight': 16,\n",
    "                               'colsample_bytree': 0.4893394195489041,\n",
    "                               'reg_alpha': 0.18334253987924942,\n",
    "                               'reg_lambda': 0.8328414321738785,\n",
    "                               'learning_rate': 0.01\n",
    "                              }),\n",
    "             False],\n",
    "    'LGBM4': [LGBMRegressor(**{'objective': 'regression_l2',\n",
    "                               'metric': 'mape', \n",
    "                               'max_depth': 12, \n",
    "                               'num_leaves': 878,\n",
    "                               'min_child_samples': 29,\n",
    "                               'min_child_weight': 14,\n",
    "                               'colsample_bytree': 0.49788260207319734, \n",
    "                               'reg_alpha': 0.4747476308475839, \n",
    "                               'reg_lambda': 0.6960820486441526,\n",
    "                               'random_state': Config.state,\n",
    "                               'early_stopping_round': Config.early_stop,\n",
    "                               'categorical_feature': cat_features,\n",
    "                               'verbose': -1,\n",
    "                               'boosting_type': 'gbdt',\n",
    "                               'eval_metric': 'mape',\n",
    "                               'objective': 'regression_l2',\n",
    "                               'n_estimators': 3000,\n",
    "                               'learning_rate': 0.01,\n",
    "                               }),\n",
    "              False],\n",
    "    'LGBM5': [LGBMRegressor(**{'objective': 'regression_l2',\n",
    "                               'metric': 'mape', \n",
    "                               'max_depth': 7,\n",
    "                               'num_leaves': 123, \n",
    "                               'min_child_samples': 21,\n",
    "                               'min_child_weight': 24,\n",
    "                               'colsample_bytree': 0.3641261996760593, \n",
    "                               'reg_alpha': 0.03632800166349373, \n",
    "                               'reg_lambda': 0.5287861861476272,\n",
    "                               'random_state': Config.state,\n",
    "                               'early_stopping_round': Config.early_stop,\n",
    "                               'categorical_feature': cat_features,\n",
    "                               'verbose': -1,\n",
    "                               'boosting_type': 'gbdt',\n",
    "                               'n_estimators': 3000,\n",
    "                               'learning_rate': 0.01,\n",
    "                               }),\n",
    "              False],\n",
    "    'LGBM6': [LGBMRegressor(**{'objective': 'regression_l2',\n",
    "                               'metric': 'mape',\n",
    "                               'max_depth': 6,\n",
    "                               'num_leaves': 502,\n",
    "                               'min_child_samples': 23,\n",
    "                               'min_child_weight': 18, \n",
    "                               'colsample_bytree': 0.4714820876493163, \n",
    "                               'reg_alpha': 0.054972003081022576, \n",
    "                               'reg_lambda': 0.5774608955362155,\n",
    "                               'random_state': Config.state,\n",
    "                               'early_stopping_round': Config.early_stop,\n",
    "                               'categorical_feature': cat_features,\n",
    "                               'verbose': -1,\n",
    "                               'boosting_type': 'goss',\n",
    "                               'n_estimators': 3000,\n",
    "                               'learning_rate': 0.01,\n",
    "                              }),\n",
    "             False],\n",
    "    'LGBM7': [LGBMRegressor(**{'objective': 'regression_l2', \n",
    "                               'metric': 'mape',\n",
    "                               'max_depth': 14,\n",
    "                               'num_leaves': 279,\n",
    "                               'min_child_samples': 7,\n",
    "                               'min_child_weight': 24, \n",
    "                               'colsample_bytree': 0.43218993309765835,\n",
    "                               'reg_alpha': 0.42757392987472964,\n",
    "                               'reg_lambda': 0.9039762787446107,\n",
    "                               'random_state': Config.state,\n",
    "                               'early_stopping_round': Config.early_stop,\n",
    "                               'categorical_feature': cat_features,\n",
    "                               'verbose': -1,\n",
    "                               'boosting_type': 'goss',\n",
    "                               'n_estimators': 3000,\n",
    "                               'learning_rate': 0.01,\n",
    "                               }),\n",
    "              False],\n",
    "    'Ridge': [Ridge(tol=1e-2, max_iter=1000000,\n",
    "                    random_state=Config.state),\n",
    "              False],\n",
    "    'BRidge': [BayesianRidge(tol=1e-2, max_iter=1000000),\n",
    "              False],\n",
    "    'LR': [LinearRegression(),\n",
    "              False],\n",
    "    'HGB': [HistGradientBoostingRegressor(**{'max_depth': 4,\n",
    "                                             'loss': 'squared_error',\n",
    "                                             'l2_regularization': 0.014082438341668873,\n",
    "                                             'min_samples_leaf': 39,\n",
    "                                             'max_leaf_nodes': 25,\n",
    "                                             'learning_rate': 0.01,\n",
    "                                             'max_iter': 2000,\n",
    "                                             'random_state': Config.state,\n",
    "                                             'early_stopping': 'auto',\n",
    "                                            }),\n",
    "              True],\n",
    "    'HGB2': [HistGradientBoostingRegressor(**{'max_depth': 4,\n",
    "                                              'loss': 'squared_error',\n",
    "                                              'l2_regularization': 1.0294569289519551e-05,\n",
    "                                              'min_samples_leaf': 12, \n",
    "                                              'max_leaf_nodes': 26,\n",
    "                                              'learning_rate': 0.01,\n",
    "                                              'max_iter': 2000,\n",
    "                                              'random_state': Config.state,\n",
    "                                              'early_stopping': 'auto',\n",
    "                                             }),\n",
    "             True],\n",
    "    'HGB3': [HistGradientBoostingRegressor(**{'max_depth': 13, \n",
    "                                              'loss': 'squared_error',\n",
    "                                              'l2_regularization': 0.05253480068908677,\n",
    "                                              'min_samples_leaf': 19,\n",
    "                                              'max_leaf_nodes': 40,\n",
    "                                              'learning_rate': 0.01,\n",
    "                                              'max_iter': 3000,\n",
    "                                              'random_state': Config.state,\n",
    "                                              'early_stopping': 'auto',\n",
    "                                             }),\n",
    "             True],\n",
    "    'HGB4': [HistGradientBoostingRegressor(**{'max_depth': 4, \n",
    "                                              'loss': 'squared_error', \n",
    "                                              'l2_regularization': 1.3248236291502028e-09,\n",
    "                                              'min_samples_leaf': 39,\n",
    "                                              'max_leaf_nodes': 29,\n",
    "                                              'learning_rate': 0.01,\n",
    "                                              'max_iter': 3000,\n",
    "                                              'random_state': Config.state,\n",
    "                                              'early_stopping': 'auto',\n",
    "                                             }),\n",
    "             True],\n",
    "    'YDF': [YDFRegressor(GradientBoostedTreesLearner)({'num_trees': 1000,\n",
    "                                                       'max_depth': 13,\n",
    "                                                       }),\n",
    "            True]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Config):\n",
    "    \n",
    "    def __init__(self, X, X_enc, y, test, test_enc, models):\n",
    "        self.y = y\n",
    "        self.models = models\n",
    "        self.scores = pd.DataFrame(columns=['Score'])\n",
    "        self.OOF_preds = pd.DataFrame()\n",
    "        self.TEST_preds = pd.DataFrame()\n",
    "        self.folds = GroupKFold(n_splits=self.n_splits)\n",
    "        self.model = None\n",
    "        self.model_dir = './pgs51/models/'\n",
    "\n",
    "    def train(self):      \n",
    "        os.listdir(f'./submission_0/submissions_{model_round_}')\n",
    "        for model_name, [model, training] in tqdm(self.models.items()):\n",
    "            \n",
    "            if training:\n",
    "                print('='*20)\n",
    "                print(model_name)\n",
    "                if any(model in model_name for model in ['LGBM', 'CAT', 'XGB']):\n",
    "                    self.X = X\n",
    "                    self.test = test\n",
    " \n",
    "                else:\n",
    "                    self.X = X_enc\n",
    "                    self.test = test_enc\n",
    "                    \n",
    "                if 'NN' in model_name:\n",
    "                    for n_fold, (train_id, valid_id) in enumerate(self.folds.split(self.X, self.y, groups = X['year'])):\n",
    "\n",
    "                        X_train_cats = self.X.loc[train_id, cat_features]\n",
    "                        X_train_nums = self.X.loc[train_id, t.num_features]\n",
    "                        y_train = self.y.loc[train_id].values\n",
    "\n",
    "                        X_val_cats = self.X.loc[valid_id, cat_features]\n",
    "                        X_val_nums = self.X.loc[valid_id, t.num_features]\n",
    "                        y_val = self.y.loc[valid_id]\n",
    "\n",
    "                        X_test_cats = self.test[cat_features]\n",
    "                        X_test_nums = self.test[t.num_features]\n",
    "        \n",
    "                        oof_preds = pd.DataFrame(columns=[model_name], index=X_val_cats.index)\n",
    "                        test_preds = pd.DataFrame(columns=[model_name], index=test.index)\n",
    "                        print(f'Fold {n_fold+1}')\n",
    "                        \n",
    "                        model = build_model_attention() #build_model()                        \n",
    "                        keras.utils.set_random_seed(self.state)\n",
    "                        optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "                        model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "                        model.fit([X_train_cats,X_train_nums], y_train, \n",
    "                                  validation_data=([X_val_cats, X_val_nums], y_val),\n",
    "                                  epochs=20,\n",
    "                                  batch_size=1000,\n",
    "                                  callbacks=[keras.callbacks.ReduceLROnPlateau(patience=1),\n",
    "                                             keras.callbacks.EarlyStopping(patience=3)\n",
    "                                            ])\n",
    "                        \n",
    "                        y_pred_val = model.predict([X_val_cats, X_val_nums])\n",
    "                        # print(X_train_cats.shape, X_train_nums.shape)\n",
    "                        # print(X_test_cats.shape, X_test_nums.shape)\n",
    "                        test_pred = model.predict([X_test_cats, X_test_nums])\n",
    "                        \n",
    "                        score = mean_absolute_percentage_error(y_val, y_pred_val)\n",
    "                        print(score)\n",
    "                        self.scores.loc[f'{model_name}', f'Fold {n_fold+1}'] = score\n",
    "                        \n",
    "                        oof_preds[model_name] = y_pred_val\n",
    "                        test_preds[model_name] = test_pred\n",
    "\n",
    "                        self.OOF_preds  = pd.concat([self.OOF_preds, oof_preds], axis = 0, ignore_index = False)\n",
    "                        self.TEST_preds = pd.concat([self.TEST_preds, test_preds], axis = 0, ignore_index = False)\n",
    "                                    \n",
    "                        # Save the model to a file locally\n",
    "                        model_save_path = f\"{self.model_dir}/{model_name}_fold{n_fold+1}.h5\"\n",
    "                        # model.save(model_save_path)\n",
    "                else:\n",
    "                    for n_fold, (train_id, valid_id) in enumerate(self.folds.split(self.X, self.y, groups = self.X['year'])):\n",
    "                        X_train, y_train = self.X.iloc[train_id], self.y.iloc[train_id]\n",
    "                        X_val, y_val = self.X.iloc[valid_id], self.y.iloc[valid_id]\n",
    "\n",
    "                        oof_preds = pd.DataFrame(columns=[model_name], index=X_val.index)\n",
    "                        test_preds = pd.DataFrame(columns=[model_name], index=test.index)\n",
    "                        print(f'Fold {n_fold+1}')\n",
    "\n",
    "                        if \"XGB\" in model_name:\n",
    "                            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "                            self.model = model\n",
    "                            # Save XGBoost model\n",
    "                            model_save_path = f\"{self.model_dir}/{model_name}_fold{n_fold+1}.json\"\n",
    "                            # model.save_model(model_save_path)\n",
    "                \n",
    "                        elif \"CAT\" in model_name:\n",
    "                            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "                            # Save CatBoost model\n",
    "                            self.model = model\n",
    "                            model_save_path = f\"{self.model_dir}/{model_name}_fold{n_fold+1}.cbm\"\n",
    "                            # model.save_model(model_save_path)\n",
    "                \n",
    "                        elif \"LGBM\" in model_name:\n",
    "                            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[log_evaluation(0), early_stopping(self.early_stop, verbose=False)])\n",
    "                            self.model = model\n",
    "                            # Save LightGBM model\n",
    "                            model_save_path = f\"{self.model_dir}/{model_name}_fold{n_fold+1}.txt\"\n",
    "                            # model.booster_.save_model(model_save_path)  # Use the booster_ attribute\n",
    "                \n",
    "                        else:\n",
    "                            model.fit(X_train, y_train)\n",
    "                            # Save general model (if applicable, e.g., a neural network)\n",
    "                            self.model = model\n",
    "                            model_save_path = f\"{self.model_dir}/{model_name}_fold{n_fold+1}.h5\"\n",
    "                            # model.save(model_save_path)\n",
    "\n",
    "                        y_pred_val = model.predict(X_val)\n",
    "                        test_pred = model.predict(self.test)\n",
    "                       \n",
    "                        score = mean_absolute_percentage_error(np.expm1(y_val), np.expm1(y_pred_val))\n",
    "                        print(score)\n",
    "                        self.scores.loc[f'{model_name}', f'Fold {n_fold+1}'] = score\n",
    "\n",
    "                        oof_preds[model_name] = y_pred_val\n",
    "                        test_preds[model_name] = test_pred\n",
    "                        self.OOF_preds = pd.concat([self.OOF_preds, oof_preds], axis = 0, ignore_index = False)\n",
    "                        self.TEST_preds = pd.concat([self.TEST_preds, test_preds], axis = 0, ignore_index = False)\n",
    "\n",
    "                self.OOF_preds = self.OOF_preds.groupby(level=0).mean()\n",
    "                self.TEST_preds = self.TEST_preds.groupby(level=0).mean()\n",
    "\n",
    "                self.OOF_preds[f'{model_name}'].to_csv(f'./submission_0/submissions_{model_round_}/{model_name}_oof.csv', index=False)\n",
    "                self.TEST_preds[f'{model_name}'].to_csv(f'./submission_0/submissions_{model_round_}/{model_name}_test.csv', index=False)\n",
    "            \n",
    "            else:\n",
    "                self.OOF_preds[f'{model_name}'] = pd.read_csv(f'/kaggle/input/sticker-models/{model_name}_oof.csv')\n",
    "                self.TEST_preds[f'{model_name}'] = pd.read_csv(f'/kaggle/input/sticker-models/{model_name}_test.csv')\n",
    "                \n",
    "                for n_fold, (train_id, valid_id) in enumerate(self.folds.split(self.OOF_preds[f'{model_name}'], self.y, groups = X['year'])):\n",
    "                    y_pred_val, y_val = self.OOF_preds[f'{model_name}'].iloc[valid_id], self.y.iloc[valid_id]\n",
    "                    self.scores.loc[f'{model_name}', f'Fold {n_fold+1}'] = mean_absolute_percentage_error(np.expm1(y_val), np.expm1(y_pred_val))\n",
    "                    \n",
    "            self.scores.loc[f'{model_name}', 'Score'] = self.scores.loc[f'{model_name}'][1:].mean()\n",
    "        self.scores.loc['Ensemble'], self.OOF_preds[\"Ensemble\"], self.TEST_preds[\"Ensemble\"] = self.ensemble(self.OOF_preds, self.y, self.TEST_preds)\n",
    "        self.scores = self.scores.sort_values('Score')\n",
    "\n",
    "        self.result()\n",
    "\n",
    "        return self.TEST_preds\n",
    "    \n",
    "    def ensemble(self, X, y, test):\n",
    "        scores = []\n",
    "        oof_pred = np.zeros(X.shape[0])\n",
    "        test_pred = np.zeros(test.shape[0])\n",
    "        model = BayesianRidge()\n",
    "        kf = KFold(n_splits=self.n_splits, random_state=self.state, shuffle=True)\n",
    "        \n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_pred_probs = model.predict(X_val)\n",
    "            oof_pred[val_idx] = y_pred_probs\n",
    "            test_pred += model.predict(test) / self.n_splits\n",
    "            \n",
    "            score = mean_absolute_percentage_error(np.expm1(y_val), np.expm1(y_pred_probs))\n",
    "            scores.append(score)\n",
    "                   \n",
    "        return np.mean(scores), oof_pred, test_pred\n",
    "    \n",
    "    def result(self):\n",
    "               \n",
    "        plt.figure(figsize=(14, 6))\n",
    "        colors = ['#3cb371' if i != 'Ensemble' else 'r' for i in self.scores.Score.index]\n",
    "        hbars = plt.barh(self.scores.index, self.scores.Score, color=colors, height=0.8)\n",
    "        plt.bar_label(hbars, fmt='%.5f')\n",
    "        plt.ylabel('Models')\n",
    "        plt.xlabel('Score')              \n",
    "        plt.show()\n",
    "\n",
    "        y = np.expm1(self.y).sort_index()\n",
    "        self.OOF_preds['Ensemble'] = np.expm1(self.OOF_preds['Ensemble']).sort_index()\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        axes[0].scatter(y, self.OOF_preds['Ensemble'], alpha=0.5, s=15, edgecolors='#3cb371')\n",
    "        axes[0].plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\n",
    "        axes[0].set_xlabel('Actual')\n",
    "        axes[0].set_ylabel('Predicted')\n",
    "        axes[0].set_title('Actual vs. Predicted')\n",
    "\n",
    "        axes[1].scatter(self.OOF_preds['Ensemble'], y - self.OOF_preds['Ensemble'], alpha=0.5, s=15, edgecolors='#3cb371')\n",
    "        axes[1].axhline(y=0, color='black', linestyle='--', lw=2)\n",
    "        axes[1].set_xlabel('Predicted Values')\n",
    "        axes[1].set_ylabel('Residuals')\n",
    "        axes[1].set_title('Residual Plot')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8c020bf6774518a8d5cc64a5dafbc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "NN\n",
      "Fold 1\n",
      "Epoch 1/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 41ms/step - loss: 21.1592 - val_loss: 0.5430 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 34ms/step - loss: 0.0902 - val_loss: 0.1463 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 34ms/step - loss: 0.0614 - val_loss: 0.1189 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - loss: 0.0575 - val_loss: 0.1323 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 34ms/step - loss: 0.0288 - val_loss: 0.0648 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - loss: 0.0198 - val_loss: 0.0677 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - loss: 0.0183 - val_loss: 0.0606 - learning_rate: 1.0000e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - loss: 0.0180 - val_loss: 0.0606 - learning_rate: 1.0000e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - loss: 0.0177 - val_loss: 0.0599 - learning_rate: 1.0000e-06\n",
      "Epoch 10/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 38ms/step - loss: 0.0177 - val_loss: 0.0599 - learning_rate: 1.0000e-06\n",
      "Epoch 11/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - loss: 0.0177 - val_loss: 0.0600 - learning_rate: 1.0000e-07\n",
      "Epoch 12/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - loss: 0.0177 - val_loss: 0.0600 - learning_rate: 1.0000e-08\n",
      "Epoch 13/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 40ms/step - loss: 0.0177 - val_loss: 0.0600 - learning_rate: 1.0000e-09\n",
      "\u001b[1m1030/1030\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step\n",
      "\u001b[1m3080/3080\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step\n",
      "0.037187911570072174\n",
      "Fold 2\n",
      "Epoch 1/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 40ms/step - loss: 21.0834 - val_loss: 0.2462 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 34ms/step - loss: 0.0883 - val_loss: 0.0720 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - loss: 0.0645 - val_loss: 0.0436 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - loss: 0.0488 - val_loss: 0.0550 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - loss: 0.0269 - val_loss: 0.0276 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - loss: 0.0201 - val_loss: 0.0269 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - loss: 0.0198 - val_loss: 0.0264 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - loss: 0.0195 - val_loss: 0.0257 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - loss: 0.0192 - val_loss: 0.0247 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - loss: 0.0188 - val_loss: 0.0237 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 34ms/step - loss: 0.0185 - val_loss: 0.0232 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 34ms/step - loss: 0.0182 - val_loss: 0.0228 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - loss: 0.0180 - val_loss: 0.0223 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - loss: 0.0177 - val_loss: 0.0217 - learning_rate: 1.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - loss: 0.0175 - val_loss: 0.0212 - learning_rate: 1.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 34ms/step - loss: 0.0172 - val_loss: 0.0208 - learning_rate: 1.0000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 34ms/step - loss: 0.0169 - val_loss: 0.0205 - learning_rate: 1.0000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - loss: 0.0166 - val_loss: 0.0204 - learning_rate: 1.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 42ms/step - loss: 0.0145 - val_loss: 0.0217 - learning_rate: 1.0000e-05\n",
      "Epoch 20/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 37ms/step - loss: 0.0140 - val_loss: 0.0226 - learning_rate: 1.0000e-06\n",
      "\u001b[1m1030/1030\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step\n",
      "\u001b[1m3080/3080\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step\n",
      "0.023581016808748245\n",
      "Fold 3\n",
      "Epoch 1/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 48ms/step - loss: 23.1189 - val_loss: 0.6018 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.0951 - val_loss: 0.0959 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - loss: 0.0725 - val_loss: 0.0785 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 43ms/step - loss: 0.0654 - val_loss: 0.1123 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.0318 - val_loss: 0.0297 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.0210 - val_loss: 0.0300 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.0191 - val_loss: 0.0269 - learning_rate: 1.0000e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.0189 - val_loss: 0.0269 - learning_rate: 1.0000e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.0187 - val_loss: 0.0268 - learning_rate: 1.0000e-06\n",
      "Epoch 10/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.0187 - val_loss: 0.0268 - learning_rate: 1.0000e-06\n",
      "Epoch 11/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.0186 - val_loss: 0.0267 - learning_rate: 1.0000e-07\n",
      "Epoch 12/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - loss: 0.0186 - val_loss: 0.0267 - learning_rate: 1.0000e-08\n",
      "Epoch 13/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.0186 - val_loss: 0.0267 - learning_rate: 1.0000e-09\n",
      "Epoch 14/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - loss: 0.0186 - val_loss: 0.0267 - learning_rate: 1.0000e-10\n",
      "Epoch 15/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.0186 - val_loss: 0.0267 - learning_rate: 1.0000e-11\n",
      "Epoch 16/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.0186 - val_loss: 0.0267 - learning_rate: 1.0000e-12\n",
      "Epoch 17/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - loss: 0.0186 - val_loss: 0.0267 - learning_rate: 1.0000e-13\n",
      "Epoch 18/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.0186 - val_loss: 0.0267 - learning_rate: 1.0000e-14\n",
      "Epoch 19/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.0186 - val_loss: 0.0267 - learning_rate: 1.0000e-15\n",
      "\u001b[1m2054/2054\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step\n",
      "\u001b[1m3080/3080\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\n",
      "0.02551070787012577\n",
      "Fold 4\n",
      "Epoch 1/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 47ms/step - loss: 23.1769 - val_loss: 0.5060 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 39ms/step - loss: 0.0907 - val_loss: 0.0992 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 39ms/step - loss: 0.0672 - val_loss: 0.0852 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step - loss: 0.0613 - val_loss: 0.0783 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - loss: 0.0535 - val_loss: 0.1264 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 39ms/step - loss: 0.0292 - val_loss: 0.0376 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step - loss: 0.0188 - val_loss: 0.0357 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - loss: 0.0184 - val_loss: 0.0353 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - loss: 0.0180 - val_loss: 0.0354 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - loss: 0.0166 - val_loss: 0.0389 - learning_rate: 1.0000e-05\n",
      "Epoch 11/20\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - loss: 0.0162 - val_loss: 0.0378 - learning_rate: 1.0000e-06\n",
      "\u001b[1m2054/2054\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "\u001b[1m3080/3080\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\n",
      "0.03036496601998806\n",
      "Fold 5\n",
      "Epoch 1/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - loss: 20.9805 - val_loss: 0.5006 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - loss: 0.0832 - val_loss: 0.0684 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - loss: 0.0640 - val_loss: 0.0579 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - loss: 0.0605 - val_loss: 0.0776 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 37ms/step - loss: 0.0263 - val_loss: 0.0269 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - loss: 0.0199 - val_loss: 0.0279 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - loss: 0.0183 - val_loss: 0.0227 - learning_rate: 1.0000e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - loss: 0.0179 - val_loss: 0.0226 - learning_rate: 1.0000e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - loss: 0.0179 - val_loss: 0.0225 - learning_rate: 1.0000e-05\n",
      "Epoch 10/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - loss: 0.0178 - val_loss: 0.0231 - learning_rate: 1.0000e-06\n",
      "Epoch 11/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 38ms/step - loss: 0.0176 - val_loss: 0.0231 - learning_rate: 1.0000e-07\n",
      "Epoch 12/20\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 34ms/step - loss: 0.0176 - val_loss: 0.0231 - learning_rate: 1.0000e-08\n",
      "\u001b[1m1027/1027\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step\n",
      "\u001b[1m3080/3080\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\n",
      "0.024288088083267212\n",
      "====================\n",
      "LGBM2\n",
      "Fold 1\n",
      "0.05377277289749689\n",
      "Fold 2\n",
      "0.06392782246348085\n",
      "Fold 3\n",
      "0.076821764255633\n",
      "Fold 4\n",
      "0.10384320409450595\n",
      "Fold 5\n",
      "0.054728217283720085\n",
      "====================\n",
      "LGBM3\n",
      "Fold 1\n",
      "0.054504786207266634\n",
      "Fold 2\n",
      "0.06122043901714811\n",
      "Fold 3\n",
      "0.0773718419798672\n",
      "Fold 4\n",
      "0.10446820372496304\n",
      "Fold 5\n",
      "0.05271086894376894\n",
      "====================\n",
      "LGBM4\n",
      "Fold 1\n",
      "0.05378265789734512\n",
      "Fold 2\n",
      "0.06479232246683674\n",
      "Fold 3\n",
      "0.07690780797940909\n",
      "Fold 4\n",
      "0.10376001311471648\n",
      "Fold 5\n",
      "0.05462619073977627\n",
      "====================\n",
      "LGBM5\n",
      "Fold 1\n",
      "0.05643953609411348\n",
      "Fold 2\n",
      "0.06269279563169068\n",
      "Fold 3\n",
      "0.07581163027605768\n",
      "Fold 4\n",
      "0.10418811315964935\n",
      "Fold 5\n",
      "0.052807862061331916\n",
      "====================\n",
      "LGBM6\n",
      "Fold 1\n",
      "0.054260412770930605\n",
      "Fold 2\n",
      "0.062853931154631\n",
      "Fold 3\n",
      "0.07705157983521776\n",
      "Fold 4\n",
      "0.10407638688771294\n",
      "Fold 5\n",
      "0.05319626159365329\n",
      "====================\n",
      "LGBM7\n",
      "Fold 1\n",
      "0.053583734334458615\n",
      "Fold 2\n",
      "0.06389002223534203\n",
      "Fold 3\n",
      "0.07711932687263771\n",
      "Fold 4\n",
      "0.10244109257009738\n",
      "Fold 5\n",
      "0.05486211436417946\n",
      "====================\n",
      "Ridge\n",
      "Fold 1\n",
      "0.33282641478491576\n",
      "Fold 2\n",
      "0.5929015367584778\n",
      "Fold 3\n",
      "0.4735029765504866\n",
      "Fold 4\n",
      "0.5610676162572928\n",
      "Fold 5\n",
      "0.5186168664120367\n",
      "====================\n",
      "BRidge\n",
      "Fold 1\n",
      "0.3719538028230497\n",
      "Fold 2\n",
      "0.5996658611414688\n",
      "Fold 3\n",
      "0.4575163798292032\n",
      "Fold 4\n",
      "0.5145919013271251\n",
      "Fold 5\n",
      "0.49282441152556494\n",
      "====================\n",
      "LR\n",
      "Fold 1\n",
      "0.3720696035674321\n",
      "Fold 2\n",
      "0.6002034831964441\n",
      "Fold 3\n",
      "0.4575285834141329\n",
      "Fold 4\n",
      "0.5131649521016439\n",
      "Fold 5\n",
      "0.4927353042715564\n",
      "====================\n",
      "HGB\n",
      "Fold 1\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'early_stopping' parameter of HistGradientBoostingRegressor must be a str among {'auto'} or an instance of 'bool' or an instance of 'numpy.bool_'. Got 200 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[131], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(X, X_enc, y, test, test_enc, models)\n\u001b[1;32m----> 2\u001b[0m TEST_preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[130], line 107\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m     model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_fold\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;66;03m# model.booster_.save_model(model_save_path)  # Use the booster_ attribute\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m# Save general model (if applicable, e.g., a neural network)\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n",
      "File \u001b[1;32md:\\Installs\\miniconda\\envs\\devp\\Lib\\site-packages\\sklearn\\base.py:1382\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1377\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1378\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1379\u001b[0m )\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1382\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[0;32m   1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Installs\\miniconda\\envs\\devp\\Lib\\site-packages\\sklearn\\base.py:436\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    429\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 436\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Installs\\miniconda\\envs\\devp\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:98\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     96\u001b[0m     )\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'early_stopping' parameter of HistGradientBoostingRegressor must be a str among {'auto'} or an instance of 'bool' or an instance of 'numpy.bool_'. Got 200 instead."
     ]
    }
   ],
   "source": [
    "model = Model(X, X_enc, y, test, test_enc, models)\n",
    "TEST_preds = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NN</th>\n",
       "      <th>LGBM2</th>\n",
       "      <th>LGBM3</th>\n",
       "      <th>LGBM4</th>\n",
       "      <th>LGBM5</th>\n",
       "      <th>LGBM6</th>\n",
       "      <th>LGBM7</th>\n",
       "      <th>Ridge</th>\n",
       "      <th>BRidge</th>\n",
       "      <th>LR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.129717</td>\n",
       "      <td>4.927054</td>\n",
       "      <td>4.932944</td>\n",
       "      <td>4.928423</td>\n",
       "      <td>4.934938</td>\n",
       "      <td>4.945224</td>\n",
       "      <td>4.931764</td>\n",
       "      <td>4.946635</td>\n",
       "      <td>4.998052</td>\n",
       "      <td>4.999428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.781187</td>\n",
       "      <td>6.793692</td>\n",
       "      <td>6.803356</td>\n",
       "      <td>6.806989</td>\n",
       "      <td>6.803409</td>\n",
       "      <td>6.818374</td>\n",
       "      <td>6.800581</td>\n",
       "      <td>6.884230</td>\n",
       "      <td>6.935839</td>\n",
       "      <td>6.937215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.389798</td>\n",
       "      <td>6.523429</td>\n",
       "      <td>6.532944</td>\n",
       "      <td>6.532189</td>\n",
       "      <td>6.531357</td>\n",
       "      <td>6.539062</td>\n",
       "      <td>6.532437</td>\n",
       "      <td>6.482256</td>\n",
       "      <td>6.533808</td>\n",
       "      <td>6.535185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.105766</td>\n",
       "      <td>5.990121</td>\n",
       "      <td>6.005228</td>\n",
       "      <td>5.990044</td>\n",
       "      <td>5.997787</td>\n",
       "      <td>6.013825</td>\n",
       "      <td>5.998052</td>\n",
       "      <td>5.955516</td>\n",
       "      <td>6.006999</td>\n",
       "      <td>6.008376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.172185</td>\n",
       "      <td>6.147680</td>\n",
       "      <td>6.139400</td>\n",
       "      <td>6.149976</td>\n",
       "      <td>6.151155</td>\n",
       "      <td>6.154863</td>\n",
       "      <td>6.150985</td>\n",
       "      <td>6.218013</td>\n",
       "      <td>6.269510</td>\n",
       "      <td>6.270886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98545</th>\n",
       "      <td>5.506549</td>\n",
       "      <td>5.959079</td>\n",
       "      <td>5.938867</td>\n",
       "      <td>5.954449</td>\n",
       "      <td>5.952507</td>\n",
       "      <td>5.943346</td>\n",
       "      <td>5.951212</td>\n",
       "      <td>5.260140</td>\n",
       "      <td>5.503418</td>\n",
       "      <td>5.510264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98546</th>\n",
       "      <td>7.202381</td>\n",
       "      <td>7.796944</td>\n",
       "      <td>7.817928</td>\n",
       "      <td>7.807743</td>\n",
       "      <td>7.809929</td>\n",
       "      <td>7.826960</td>\n",
       "      <td>7.807891</td>\n",
       "      <td>7.210986</td>\n",
       "      <td>7.453045</td>\n",
       "      <td>7.459886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98547</th>\n",
       "      <td>7.033309</td>\n",
       "      <td>7.653188</td>\n",
       "      <td>7.666553</td>\n",
       "      <td>7.667678</td>\n",
       "      <td>7.684306</td>\n",
       "      <td>7.674638</td>\n",
       "      <td>7.659074</td>\n",
       "      <td>7.071884</td>\n",
       "      <td>7.314123</td>\n",
       "      <td>7.320965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98548</th>\n",
       "      <td>6.394342</td>\n",
       "      <td>7.016774</td>\n",
       "      <td>7.028722</td>\n",
       "      <td>7.024441</td>\n",
       "      <td>7.031723</td>\n",
       "      <td>7.031981</td>\n",
       "      <td>7.021231</td>\n",
       "      <td>6.309920</td>\n",
       "      <td>6.552756</td>\n",
       "      <td>6.559600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98549</th>\n",
       "      <td>6.489151</td>\n",
       "      <td>7.144430</td>\n",
       "      <td>7.141751</td>\n",
       "      <td>7.151838</td>\n",
       "      <td>7.166251</td>\n",
       "      <td>7.148916</td>\n",
       "      <td>7.158667</td>\n",
       "      <td>6.573347</td>\n",
       "      <td>6.816093</td>\n",
       "      <td>6.822937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98550 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             NN     LGBM2     LGBM3     LGBM4     LGBM5     LGBM6     LGBM7   \n",
       "0      5.129717  4.927054  4.932944  4.928423  4.934938  4.945224  4.931764  \\\n",
       "1      6.781187  6.793692  6.803356  6.806989  6.803409  6.818374  6.800581   \n",
       "2      6.389798  6.523429  6.532944  6.532189  6.531357  6.539062  6.532437   \n",
       "3      6.105766  5.990121  6.005228  5.990044  5.997787  6.013825  5.998052   \n",
       "4      6.172185  6.147680  6.139400  6.149976  6.151155  6.154863  6.150985   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "98545  5.506549  5.959079  5.938867  5.954449  5.952507  5.943346  5.951212   \n",
       "98546  7.202381  7.796944  7.817928  7.807743  7.809929  7.826960  7.807891   \n",
       "98547  7.033309  7.653188  7.666553  7.667678  7.684306  7.674638  7.659074   \n",
       "98548  6.394342  7.016774  7.028722  7.024441  7.031723  7.031981  7.021231   \n",
       "98549  6.489151  7.144430  7.141751  7.151838  7.166251  7.148916  7.158667   \n",
       "\n",
       "          Ridge    BRidge        LR  \n",
       "0      4.946635  4.998052  4.999428  \n",
       "1      6.884230  6.935839  6.937215  \n",
       "2      6.482256  6.533808  6.535185  \n",
       "3      5.955516  6.006999  6.008376  \n",
       "4      6.218013  6.269510  6.270886  \n",
       "...         ...       ...       ...  \n",
       "98545  5.260140  5.503418  5.510264  \n",
       "98546  7.210986  7.453045  7.459886  \n",
       "98547  7.071884  7.314123  7.320965  \n",
       "98548  6.309920  6.552756  6.559600  \n",
       "98549  6.573347  6.816093  6.822937  \n",
       "\n",
       "[98550 rows x 10 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.TEST_preds.drop_duplicates()\n",
    "#.to_csv('submission_attention_with_extra_features.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
